# sparse_attention

注意力机制 pytorch 版本实现

1-Attention 注意力机制

2-MultiAttention 多头注意力机制

3-MultiSelfAttention 多头自注意力机制

4-AtrousSelfAttention 空洞多头注意力机制

5-LocalSelfAttention 局部多头注意力机制

6-SparseSelfAttention 稀疏多头注意力机制

参考: https://github.com/bojone/attention
